{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ab71728b2e6544376d0f7b7ce3a2690cdf3a078b6ef92caeda64ccdf9bfd26c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "from scipy import stats\n",
    "import math\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.ma as ma\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def agg(x, L, a=[]):\n",
    "    if(L == 1):\n",
    "        a = np.logical_not(ma.getmaskarray(x))\n",
    "        return np.average(np.abs(x),weights=a,axis=1)\n",
    "    if(L == 2):\n",
    "        a = np.logical_not(ma.getmaskarray(x))\n",
    "        return LA.norm(x,axis=1)/np.sqrt(np.sum(a,axis=1))\n",
    "    if(L == 3):\n",
    "        a = np.logical_not(ma.getmaskarray(x))\n",
    "        return np.prod(np.power(x,a/np.repeat(np.expand_dims(np.sum(a,axis=1),axis=1),a.shape[1],axis=1)),axis=1)\n",
    "    if(L == 4):\n",
    "        a = a*np.logical_not(ma.getmaskarray(x))\n",
    "        return np.average(np.abs(x),weights=a,axis=1)\n",
    "    if(L == 5):\n",
    "        a = a*np.logical_not(ma.getmaskarray(x))\n",
    "        return np.prod(np.power(x,a/np.repeat(np.expand_dims(np.sum(a,axis=1),axis=1),a.shape[1],axis=1)),axis=1)\n",
    "\n",
    "def all_aggs(in_chan, out_chan, in_weight, out_weight):\n",
    "    return np.asarray([agg(np.ma.concatenate((in_chan,out_chan),axis=1),L=i,a=np.ma.concatenate((in_weight,out_weight),axis=1)) for i in range(1,6)])\n",
    "\n",
    "def sqrtlog(chans, weights):\n",
    "    a = np.logical_not(ma.getmaskarray(weights))\n",
    "    all = []\n",
    "    for i in range(len(chans)):\n",
    "        all.append([])\n",
    "        chand = chans[i]*np.sum(a,axis=1)\n",
    "        all[i].append(chans[i])\n",
    "        all[i].append(chand)\n",
    "        all[i].append(np.sqrt(chans[i]))\n",
    "        all[i].append(np.sqrt(chand))\n",
    "        all[i].append(np.log(chans[i]))\n",
    "        all[i].append(np.log(chand))\n",
    "        all[i].append(np.log(np.sqrt(chans[i])))\n",
    "        all[i].append(np.log(np.sqrt(chand)))\n",
    "    return all\n",
    "\n",
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename,skip_blank_lines=False)\n",
    "    data = dict()\n",
    "\n",
    "    if(pd.isna(df.iloc[-1][1])):\n",
    "        df = df.drop(labels=df.shape[0]-1, axis=0)\n",
    "\n",
    "    zero_models = []\n",
    "\n",
    "    idx = list(np.where(pd.isna(df[\"model_id\"]))[0])\n",
    "    idxcopy = idx\n",
    "    idx = idx - np.arange(0,len(idx),1)\n",
    "    lenidx = np.append(idx,len(df[\"model_id\"])-len(idx))\n",
    "    lenidx = np.insert(lenidx,0,0)\n",
    "    maxLength = np.max(np.abs(np.diff(lenidx)))\n",
    "    for key in list(df.keys()):\n",
    "        idxx = list(np.where(pd.isna(df[key]))[0])\n",
    "        idxx = list(set(idxcopy) ^ set(idxx))\n",
    "        data[key] = df[key]\n",
    "        datacopy = data[key]\n",
    "        data[key].loc[idxx] = 0\n",
    "        data[key] = data[key].dropna(axis=0)\n",
    "        data[key] = np.array_split(data[key],idx)\n",
    "    \n",
    "        for i in range(len(data[key])):\n",
    "            #equalize all model sizes\n",
    "            prevlen = len(data[key][i])\n",
    "            data[key][i] = np.append(data[key][i],(np.zeros((maxLength-len(data[key][i])))))\n",
    "            #delete zero models\n",
    "            num_non_zero = np.sum(data[key][i]!=0)\n",
    "            threshold = 1\n",
    "            if(num_non_zero<threshold):\n",
    "                #print(key,str(i),num_non_zero,prevlen)\n",
    "                zero_models.append(i)\n",
    "        data[key] = np.asarray(data[key])\n",
    "    \n",
    "    zero_models = list(set(zero_models))\n",
    "    print(\"zero models deleted: \"+str(len(zero_models)))\n",
    "    for key in list(data.keys()):\n",
    "        data[key] = np.delete(data[key],zero_models,axis=0)\n",
    "        data[key] = ma.masked_array(data[key], mask=(data[key]==0.))\n",
    "\n",
    "    data['in_QS_BE'] = np.arctan2(data['in_S_BE'],(1-1/data['in_C_BE']))\n",
    "    data['out_QS_BE'] = np.arctan2(data['out_S_BE'],(1-1/data['out_C_BE']))\n",
    "    data['in_QS_AE'] = np.arctan2(data['in_S_AE'],(1-1/data['in_C_AE']))\n",
    "    data['out_QS_AE'] = np.arctan2(data['out_S_AE'],(1-1/data['out_C_AE']))\n",
    "\n",
    "    aggregates = dict()\n",
    "\n",
    "    aggregates['QS_BE'] = all_aggs(data['in_QS_BE'],data['out_QS_BE'],data['in_weight_BE'],data['out_weight_BE'])\n",
    "    aggregates['QS_AE'] = all_aggs(data['in_QS_AE'],data['out_QS_AE'],data['in_weight_AE'],data['out_weight_AE'])\n",
    "    aggregates['QE_BE'] = all_aggs(data['in_ER_BE'],data['out_ER_BE'],data['in_weight_BE'],data['out_weight_BE'])\n",
    "    aggregates['QE_AE'] = all_aggs(data['in_ER_AE'],data['out_ER_AE'],data['in_weight_AE'],data['out_weight_AE'])\n",
    "\n",
    "    aggregates['QS_BE'] = sqrtlog(aggregates['QS_BE'],np.ma.concatenate((data['in_weight_BE'],data['out_weight_BE']),axis=1))\n",
    "    aggregates['QS_AE'] = sqrtlog(aggregates['QS_AE'],np.ma.concatenate((data['in_weight_AE'],data['out_weight_AE']),axis=1))\n",
    "    aggregates['QE_BE'] = sqrtlog(aggregates['QE_BE'],np.ma.concatenate((data['in_weight_BE'],data['out_weight_BE']),axis=1))\n",
    "    aggregates['QE_AE'] = sqrtlog(aggregates['QE_AE'],np.ma.concatenate((data['in_weight_AE'],data['out_weight_AE']),axis=1))\n",
    "\n",
    "    aggregates['spec_BE'] = all_aggs(data['in_spec_BE'],data['out_spec_BE'],data['in_weight_BE'],data['out_weight_BE'])\n",
    "    aggregates['spec_AE'] = all_aggs(data['in_spec_AE'],data['out_spec_AE'],data['in_weight_AE'],data['out_weight_AE'])\n",
    "    aggregates['fro_BE'] = all_aggs(data['in_fro_BE'],data['out_fro_BE'],data['in_weight_BE'],data['out_weight_BE'])\n",
    "    aggregates['fro_AE'] = all_aggs(data['in_fro_AE'],data['out_fro_AE'],data['in_weight_AE'],data['out_weight_AE'])\n",
    "\n",
    "    aggregates['spec_BE'] = sqrtlog(aggregates['spec_BE'],np.ma.concatenate((data['in_weight_BE'],data['out_weight_BE']),axis=1))\n",
    "    aggregates['spec_AE'] = sqrtlog(aggregates['spec_AE'],np.ma.concatenate((data['in_weight_AE'],data['out_weight_AE']),axis=1))\n",
    "    aggregates['fro_BE'] = sqrtlog(aggregates['fro_BE'],np.ma.concatenate((data['in_weight_BE'],data['out_weight_BE']),axis=1))\n",
    "    aggregates['fro_AE'] = sqrtlog(aggregates['fro_AE'],np.ma.concatenate((data['in_weight_AE'],data['out_weight_AE']),axis=1))\n",
    "\n",
    "    aggregates['path'] = np.mean(data['path'],axis=1)\n",
    "\n",
    "    aggregates['test_acc'] = np.mean(data['test_acc'],axis=1)\n",
    "    aggregates['train_acc'] = np.mean(data['train_acc'],axis=1)\n",
    "    aggregates['test_loss'] = np.mean(data['test_loss'],axis=1)\n",
    "    aggregates['train_loss'] = np.mean(data['train_loss'],axis=1)\n",
    "    aggregates['gap'] = np.mean(data['gap'],axis=1)\n",
    "\n",
    "    return aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "zero models deleted: 108\n",
      "10\n",
      "zero models deleted: 15\n",
      "20\n",
      "zero models deleted: 15\n",
      "30\n",
      "zero models deleted: 15\n",
      "40\n",
      "zero models deleted: 15\n",
      "5\n",
      "zero models deleted: 15\n",
      "50\n",
      "zero models deleted: 15\n",
      "35\n",
      "zero models deleted: 15\n",
      "45\n",
      "zero models deleted: 15\n",
      "25\n",
      "zero models deleted: 15\n",
      "15\n",
      "zero models deleted: 15\n",
      "60\n",
      "zero models deleted: 15\n",
      "55\n",
      "zero models deleted: 15\n",
      "65\n",
      "zero models deleted: 15\n",
      "69\n",
      "zero models deleted: 15\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "dataset = 'CIFAR100'\n",
    "optimizer = 'SGD'\n",
    "\n",
    "input_path = str(sys.path[0][0:-8])+\"/outputs/LilJon-\"+optimizer+\"-\"+dataset\n",
    "epochs = glob.glob(input_path+\"/*\")\n",
    "wanted_epochs = np.arange(0,70,5)\n",
    "wanted_epochs = np.append(wanted_epochs, 69)\n",
    "\n",
    "translation = {'QS':'SQ','QE':'E','spec':'S','fro':'F'}\n",
    "fancytranslation = {'QS_AE':'${\\widehat{Q}_{SQ}^{p}}$','QE_AE':'${\\widehat{Q}_{E}^{L2}}$','spec_AE':'${\\widehat{Q}_{S}^{p}}$','fro_AE':'${\\widehat{Q}_{F}^{p}}$','QS_BE':'${{Q}_{SQ}^{p}}$','QE_BE':'${{Q}_{E}^{L2}}$','spec_BE':'${{Q}_{S}^{p}}$','fro_BE':'${{Q}_{F}^{p}}$'}\n",
    "types = {'QS':[2,0],'QE':[1,6],'spec':[2,7],'fro':[2,7]}\n",
    "lrfdict = {'AE':'LRF','BE':''}\n",
    "gendict = {'test_acc':'test','gap':'gap'}\n",
    "fancygendict = {'test_acc':'Test Accuracy','gap':'Generalization Gap'}\n",
    "\n",
    "for epoch in epochs:\n",
    "    if int((epoch.split('-')[-1]).split('.')[0]) in wanted_epochs:\n",
    "        print(int((epoch.split('-')[-1]).split('.')[0]))\n",
    "        data = get_data(epoch)\n",
    "        \n",
    "        for key in translation.keys():\n",
    "            for lrf in lrfdict.keys():\n",
    "                for gen in gendict.keys():\n",
    "                    plt.figure(figsize=(8, 8))\n",
    "                    plt.plot(data[key+\"_\"+lrf][types[key][0]][types[key][1]],data[gen],'ro',alpha=0.5)\n",
    "                    coefficients = np.polyfit(data[key+\"_\"+lrf][types[key][0]][types[key][1]],data[gen],2)\n",
    "                    plt.plot(np.arange(min(data[key+\"_\"+lrf][types[key][0]][types[key][1]]),max(data[key+\"_\"+lrf][types[key][0]][types[key][1]]),0.001),np.polyval(coefficients,np.arange(min(data[key+\"_\"+lrf][types[key][0]][types[key][1]]),max(data[key+\"_\"+lrf][types[key][0]][types[key][1]]),0.001)), c='black', lw=4,alpha=0.85)\n",
    "                    plt.xlabel(\"Quality Measure \"+fancytranslation[key+\"_\"+lrf],fontsize=25)\n",
    "                    plt.ylabel(fancygendict[gen],fontsize=25)\n",
    "                    plt.xticks(fontsize=18)\n",
    "                    plt.yticks(fontsize=18)\n",
    "                    plt.title(fancygendict[gen]+\" over \"+fancytranslation[key+\"_\"+lrf]+\" at Epoch \"+str(int((epoch.split('-')[-1]).split('.')[0])+1),fontsize=25)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(str(sys.path[0][0:-9])+\"/results/\"+dataset+\"/\"+optimizer+\"/\"+translation[key]+\"/\"+gendict[gen]+\"/\"+lrfdict[lrf]+(epoch.split('-')[-1]).split('.')[0]+\".png\")\n",
    "                    plt.clf()\n",
    "                    plt.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}